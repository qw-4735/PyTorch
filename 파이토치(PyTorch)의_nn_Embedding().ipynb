{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "파이토치(PyTorch)의 nn.Embedding().ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGHM7/tny4/2vXefFuF4gA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qw-4735/PyTorch/blob/main/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98(PyTorch)%EC%9D%98_nn_Embedding().ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서 임베딩 벡터를 사용하는 방법\n",
        "1. **임베딩 층 (embedding layer)**을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법 : nn.Embedding() 사용하여 구현\n",
        "\n",
        "2. **미리 사전에 훈련된 임베딩 벡터(pre-trained word embedding)**들을 가져와 사용하는 방법"
      ],
      "metadata": {
        "id": "A4RzWPWlsu-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.  **임베딩 층 (embedding layer)**을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법"
      ],
      "metadata": {
        "id": "l8dgjzG1tnOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) 임베딩 층은 룩업 테이블이다."
      ],
      "metadata": {
        "id": "2O-yYrght306"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "mBHYwJs8v-sz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q8aKLQbsbdK",
        "outputId": "32ed934a-1a0b-46f3-f022-ce31fd55a84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'code': 2, 'need': 3, 'you': 4, 'know': 5, 'to': 6, 'how': 7, '<unk>': 0, '<pad>': 1}\n"
          ]
        }
      ],
      "source": [
        "train_data = 'you need to know how to code'\n",
        "\n",
        "# 중복을 제거한 단어들의 집합인 단어 집합 생성\n",
        "word_set = set(train_data.split())\n",
        "\n",
        "# 단어 집합의 각 단어에 고유한 정수 맵핑\n",
        "vocab = {word: i+2 for i, word in enumerate(word_set)}\n",
        "vocab['<unk>'] = 0\n",
        "vocab['<pad>'] = 1\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합의 크기만큼의 행을 가지는 테이블 생성\n",
        "embedding_table = torch.FloatTensor([\n",
        "                               [ 0.0,  0.0,  0.0],\n",
        "                               [ 0.0,  0.0,  0.0],\n",
        "                               [ 0.2,  0.9,  0.3],\n",
        "                               [ 0.1,  0.5,  0.7],\n",
        "                               [ 0.2,  0.1,  0.8],\n",
        "                               [ 0.4,  0.1,  0.1],\n",
        "                               [ 0.1,  0.8,  0.9],\n",
        "                               [ 0.6,  0.1,  0.1]])"
      ],
      "metadata": {
        "id": "7jGItmArvcWh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) 임베딩 층 사용하기 : nn.Embedding()을 사용할 경우"
      ],
      "metadata": {
        "id": "OOuwOX8KwnX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = 'you need to know how to code'\n",
        "\n",
        "# 중복을 제거한 단어들의 집합인 단어 집합 생성\n",
        "word_set = set(train_data.split())\n",
        "\n",
        "# 단어 집합의 각 단어에 고유한 정수 맵핑\n",
        "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}\n",
        "vocab['<unk>'] = 0\n",
        "vocab['<pad>'] = 1"
      ],
      "metadata": {
        "id": "HA5Y73Mev69e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Embedding()을 사용하여 학습가능한 임베딩 테이블을 만든다.\n",
        "import torch.nn as nn\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(vocab),   # num_embeddings : 임베딩 할 단어들의 개수 ( 단어 집합의 크기)\n",
        "                               embedding_dim =3,            # embedding_dim : 임베딩 할 벡터의 차원 (사용자가 정해주는 하이퍼 파라미터)\n",
        "                               padding_idx=1)               # padding_idx : 패딩을 위한 토큰의 인덱스를 알려줌 , 선택적으로 사용하는 인자"
      ],
      "metadata": {
        "id": "D-My7VDyw7wl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)   # 단어 집합의 크기의 행을 가지는 임베딩 테이블 생성"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb3Q_x52x7Jv",
        "outputId": "6e7bab96-d926-4560-d43e-b3d17240c366"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.5196, -0.7165,  0.7309],\n",
            "        [ 0.0000,  0.0000,  0.0000],\n",
            "        [-1.2850, -0.3948, -1.1524],\n",
            "        [-1.9137, -1.4778, -1.3510],\n",
            "        [-0.9318, -1.2556,  1.7311],\n",
            "        [ 0.3184, -0.7792,  1.1011],\n",
            "        [ 0.2000, -0.4754,  1.6249],\n",
            "        [ 0.6655,  0.3876,  0.1850]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 사전 훈련된 워드 임베딩(Pretrained Word Embedding)\n",
        "  훈련 데이터가 부족한 경우 모델에 파이토치의 nn.Embedding()으로 해당 문제에 충분히 특화된 임베딩 벡터를 만들어내는 것이 쉽지 않을 수 있다.\n",
        "\n",
        "\n",
        "이 경우, 해당 문제에 특화된 것은 아니지만 보다 일반적이고 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있다."
      ],
      "metadata": {
        "id": "6wjkDiutyePy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) IMDB 리뷰 데이터를 훈련 데이터로 사용하기\n",
        "- 실습을 위해서는 사전 훈련된 임베딩 벡터들을 맵핑시킬 대상인 훈련 데이터가 필요"
      ],
      "metadata": {
        "id": "YzjhpqvDz6HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import data, datasets"
      ],
      "metadata": {
        "id": "AlJKD1mZyDQ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch==1.8.0 torchtext==0.9.0\n",
        "\n",
        "# Reload environment\n",
        "exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "VKlA2US20wAh",
        "outputId": "6d5598c9-d317-4baf-a430-d09256429a52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 13 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets"
      ],
      "metadata": {
        "id": "uG4whRa7003x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필드 객체 정의\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)"
      ],
      "metadata": {
        "id": "pwGW8Rlt0Jls"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk3kGnu00g9d",
        "outputId": "f756312f-1637-414c-fd3c-d70c39f82e64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:04<00:00, 17.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(trainset))  # 훈련 데이터의 크기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VufDE36u1-Uh",
        "outputId": "bdc31cea-49f3-40eb-cd0f-dca021aa0c0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vars(trainset[0]))   # 첫번째 샘플 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEs4SosE2V-u",
        "outputId": "09d96ade-9b35-481b-f39a-1033b95395fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['if', 'you', 'are', 'uninitiated', 'to', 'the', 'gundam', 'world,', 'this', 'is', 'a', 'good', 'place', 'to', 'start.', 'if', 'you', 'are', 'burned', 'out', 'on', 'star', 'wars', 'or', 'star', 'trek,', 'here', 'is', 'a', 'compelling,', 'realistic', 'sci-fi', 'series', 'you', 'can', 'become', 'immersed', 'in.', 'not', 'the', 'simplistic', 'boy-saves-world-in-giant', 'robot', 'story', 'you', 'might', 'have', 'expected,', 'but', 'rather', 'a', 'complex,', 'emotionally', 'compelling', 'space', 'war', 'drama', 'where', 'the', 'line', 'between', 'the', '\"good\"', 'and', '\"bad\"', 'guys', 'is', 'decidedly', 'less', 'than', 'distinct.<br', '/><br', '/>gundam', '0080', 'focuses', 'on', 'the', 'story', 'of', 'al', 'izuruha,', 'a', 'young,', 'naive', 'boy', 'living', 'in', 'a', 'neutral', 'space', 'colony.', 'he', 'spends', 'his', 'days', 'daydreaming', 'about', 'mobile', 'suits', 'and', 'playing', 'war', 'with', 'his', 'friends.', 'during', 'the', 'course', 'of', 'this', 'series,', 'al', 'befriends', 'an', '\"enemy\"', 'soldier,', 'bernie', 'wiseman.', 'by', 'the', 'end,', 'little', 'al', 'learns', 'some', 'hard', 'lessons', 'about', 'the', 'reality', 'of', 'war', 'and', 'the', 'requisite', 'suffering', 'and', 'sacrifice.<br', '/><br', '/>i', 'loved', 'this', 'oav', 'series,', 'with', 'its', 'cool', 'mecha', 'designs,', 'involving', 'story,', 'and', 'likeable', 'characters.', 'i', 'recommend', 'this', 'series', 'to', 'anyone', 'who', 'likes', 'realistic', 'sf', 'anime,', 'or', 'to', 'those', 'who', 'think', 'anime', 'is', 'just', 'silly', 'or', 'sexy', 'entertainment.'], 'label': 'pos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) 토치텍스트를 사용한 사전 훈련된 워드 임베딩"
      ],
      "metadata": {
        "id": "8Vl2EMB72eG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-1) 사전 훈련된 Word2Vec 모델 확인하기\n",
        "- 여기서는 앞서 '영어/한국어 Word2Vec 훈련하기 챕터'에서 만들어두었던 'eng_w2v' 모델을 사용"
      ],
      "metadata": {
        "id": "em1KX_q32tZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "StNF4ZPC2bVd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = KeyedVectors.load_word2vec_format('eng_w2v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "eKBACTxr3MvX",
        "outputId": "213f9d2c-bf4b-4c5b-c0cc-f1f465771844"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cd2c284c853e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng_w2v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFER_FROM_EXTENSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eng_w2v'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C_BlUuTh4pEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}